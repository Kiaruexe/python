{"cells":[{"cell_type":"markdown","metadata":{"id":"pyTrXMRLC-4h"},"source":["###  En este ejemplo se explica cómo realizar una solicitud básica a Amazon Rekognition para detectar en una imagen: etiquetas (objetos) o las características de las caras de las personas, de forma similar a lo que hacen servicios como Google Cloud Vision, pero usando AWS Rekognition y la API REST vía HTTP.\n"]},{"cell_type":"markdown","metadata":{"id":"WIDSKarQC-4j"},"source":["<h1 style=\"color: red;\">Requisitos previos</h1>\n","\n","1. **Cuenta de AWS**: Necesitarás una cuenta de AWS para obtener las credenciales necesarias (Access Key y Secret Access Key).\n","\n","2. **Instalar dependencias**. Para este ejemplo, utilizaremos la biblioteca **requests** para hacer la solicitud HTTP y **boto3** para hacer uso de las credenciales AWS de forma programática.\n","\n","3. **Configurar tus credenciales de AWS**. Ve a la Consola de AWS y crea un usuario de IAM con permisos para Amazon Rekognition.\n","Genera un par de credenciales: Access Key ID y Secret Access Key. Anótalos, los necesitarás para firmar tus solicitudes.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LhUnGdOnC-4k"},"source":["<h3 style=\"color:blue;\">Instalamos las dependencias que necesitamos</h3>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mClp13vRC-4k","executionInfo":{"status":"ok","timestamp":1762801803583,"user_tz":-60,"elapsed":12911,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}},"outputId":"39a918bd-195d-4cc5-a075-d330f70515fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Collecting boto3\n","  Downloading boto3-1.40.69-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n","Collecting botocore<1.41.0,>=1.40.69 (from boto3)\n","  Downloading botocore-1.40.69-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.15.0,>=0.14.0 (from boto3)\n","  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.69->boto3) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.69->boto3) (1.17.0)\n","Downloading boto3-1.40.69-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.40.69-py3-none-any.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n","Successfully installed boto3-1.40.69 botocore-1.40.69 jmespath-1.0.1 s3transfer-0.14.0\n"]}],"source":["!pip install requests boto3\n"]},{"cell_type":"markdown","metadata":{"id":"Ro41Xd0iC-4m"},"source":["<h3 style=\"color:blue;\">Pasos para hacer una solicitud a la API de Amazon Rekognition</h3>\n","\n","1. **Crear el cuerpo de la solicitud**.\n","\n","2. **Adjuntarle todos los datos necesarios a la solicitud en su cabecera**\n","\n","3. **Autenticación (Firma Version 4)**. AWS utiliza un sistema de autenticación llamado Signature Version 4. Esto implica que debes firmar tus solicitudes HTTP con tus credenciales de AWS. No se puede simplemente hacer una solicitud sin firmarla.\n","\n","4. **Hacer la llamada y recibir la respuesta**\n","\n","5. **Procesar la respuesta**\n","\n","A continuación, veremos un ejemplo completo en Python de todo el proceso."]},{"cell_type":"markdown","metadata":{"id":"6MPaP0wOC-4m"},"source":["<h4 style=\"color:orange;\">Paso 0 (preparación de los datos y del entorno)</h4>\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_bDJaS3RC-4n","executionInfo":{"status":"ok","timestamp":1762801820424,"user_tz":-60,"elapsed":202,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Importamos los módulos necesarios\n","import hashlib\n","import hmac\n","import requests\n","import datetime\n","import base64\n","import json\n","from urllib.parse import urlencode"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"KiqHrkFNC-4n","executionInfo":{"status":"ok","timestamp":1762801822566,"user_tz":-60,"elapsed":17,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Especificamos nuestras credenciales de AWS (Access Key y Secret Access Key)\n","# Si estamos trabajando desde el Learner Lab, necesitaremos también el TOKEN de sesión. Todos estos datos los podemos sacar\n","# de la opción AWS Details,, una vez que hemos puesto en marcha el laboratorio\n","\n","AWS_ACCESS_KEY = 'ASIAUCOKYO6XJK44MF7U'\n","AWS_SECRET_KEY = 'BQ6PyL5hi6LANdKBZ+6yGUsop10lg92yKEEcqJFy'\n","AWS_SESSION_TOKEN = 'IQoJb3JpZ2luX2VjEEIaCXVzLXdlc3QtMiJIMEYCIQCwCyTHlXmoyalrOYxg3Bk5x4efG5ThHKNUA0eHNhDyNgIhAKaG+vDJBiEiIBf42V3oY0k49Qzm/JhWeRwZqenFu0jvKqACCAsQARoMMjgwMTM0OTczMzU4IgxSSbuRX1BdcMXWA+gq/QEP8DfbQ8UodiY0NrLQXadax7PKV50MFOGFazCihu5s5ZVXhWxMx9NT1PByCHC8LqfqMe+j+acO01WuUqvxDkb6Fx2S+C/g6RRO/5hVieLJIrXqilme737WFNKO/xb3qNYcTP4VT515eNRJYpwDHqiHEz1cOIXg73fzzpAYFh54acLgdsT3ZqdOHKrv3W+XauyzpxAGfymSAe5Jy3nmP25+umVRrmAsXqgjKBrMjTBknOSxmdO2nVTdLoA2aOOcmFH7UZ4BOKbSXDoJkEqUya/1GXOvYOsdvDRnSxv5iHcgaT44uzuAtFBxDEmfKFCNuYx0Fs9d7irFAddK8PMbMN3AyMgGOpwB2dBzA4CcZQ8kuWePZHLJD8pImLZyZ0lwdyT9je5avnbu9M9RrBC4i5bsJBw0Dh3f2l6+bDvN2z0NnkrfIhFPdPiqrtTw+k2axpQ38NrMoK/JMoyC2D0dp+ptDl0bC9I+0guac2fPCghxbBa8DMkG8Wr8/+8jIogoETebcNk+ykV8cIFumQq7BUaOTlR2K4OiXy3SOYbgVLxqvKP/'\n","REGION = 'us-east-1'  # Cambia a la región donde tienes habilitado Rekognition\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dZjjXA6oC-4n","executionInfo":{"status":"ok","timestamp":1762801824667,"user_tz":-60,"elapsed":8,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# URL del servicio Rekognition para la región us-east-1 (cambia si estás usando otra región)\n","rekognition_url = 'https://rekognition.us-east-1.amazonaws.com/'"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mL9D50PZC-4o","executionInfo":{"status":"ok","timestamp":1762801826321,"user_tz":-60,"elapsed":8,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Leer y codificar la imagen en base64\n","with open(\"cara1descarga.jpg\", \"rb\") as image_file:  # Cambiado a modo binario 'rb'\n","    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')  # Podríamos hacerlo en un único paso\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8oiobDhpC-4o"},"source":["<h4 style=\"color:orange;\">Paso 1 (crear el cuerpo de la solicitud)</h4>\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dcSqaBttC-4o","executionInfo":{"status":"ok","timestamp":1762801829495,"user_tz":-60,"elapsed":8,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Crear el cuerpo de la solicitud\n","request_body = {\n","    \"Image\": {\n","        \"Bytes\": image_base64   # Aquí le proporcinamos la imagen sobre la que vamos a trabajar. Tiene que estar codificada en base64\n","    },\n","\n","    \"Attributes\": [\"ALL\"], # Especifica que quieres todos los atributos\n","#   \"MaxLabels\": 10,       # Si quiero limitar el tamaño de la respuesta, puedo indicarle el número de etiquetas que quiero que me envíe\n","    \"MinConfidence\": 75    # Especifico la confianza mínima que debe tener esa características. Cuanto más próxima esté la confianza a 100, más seguro está el modelo de que esa característica es verdadera\n","}\n","\n","# Convertir el cuerpo de la petición a JSON\n","request_payload = json.dumps(request_body)"]},{"cell_type":"markdown","metadata":{"id":"Yz65hGXzC-4o"},"source":["<h4 style=\"color:orange;\">Paso 2 (Adjuntar todos los datos en la solicitud en su cabecera)</h4>\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BSCeLWdC-4p","executionInfo":{"status":"ok","timestamp":1762801832459,"user_tz":-60,"elapsed":6,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}},"outputId":"0b3a304c-ccbc-4cb7-d7db-175675eaf6d2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3634457793.py:6: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  t = datetime.datetime.utcnow()\n"]}],"source":["# Parámetros para la cabecera\n","\n","content_type = 'application/x-amz-json-1.1'                 # tipo de contenido que le vamos a enviar\n","\n","# Obtener la fecha actual en el formato requerido\n","t = datetime.datetime.utcnow()\n","amz_date = t.strftime('%Y%m%dT%H%M%SZ')                     # fecha y hora en que se hace la solictud\n","date_stamp = t.strftime('%Y%m%d')                           # fecha en la que se hace la solicitud en formato aaaammdd\n","\n","#amz_target = 'RekognitionService.DetectLabels'             # identificador del tipo de servicio al que quiero acceder. En este caso DetectLabels\n","amz_target = 'RekognitionService.DetectFaces'               # identificador del tipo de servicio al que quiero acceder. En este caso DetectFaces\n","\n","host = f'rekognition.{REGION}.amazonaws.com'                # identificación del nodo que da soporte al servicio\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xGz4De8XC-4p","executionInfo":{"status":"ok","timestamp":1762801835743,"user_tz":-60,"elapsed":8,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Creo la cabecera con los parámetros necesarios\n","headers = {\n","    'Content-Type': content_type,\n","    'X-Amz-Date': amz_date,\n","    'X-Amz-Target': amz_target,\n","    'Host': host,\n","    'X-Amz-Security-Token': AWS_SESSION_TOKEN  # Token de sesión. Sólo si trabajamos con AWS_TOKEN_SESSION\n","}\n"]},{"cell_type":"markdown","metadata":{"id":"aJgmMBWpC-4p"},"source":["<h4 style=\"color:orange;\">Paso 3 (Hacemos la firma autenticada de la solicitud y se la adjuntamos a la cabecera)</h4>"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hV6M7uy6C-4p","executionInfo":{"status":"ok","timestamp":1762801839119,"user_tz":-60,"elapsed":9,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Funciones necesarias para la firma de la solicitud\n","def sign(key, msg):\n","    return hmac.new(key, msg.encode('utf-8'), hashlib.sha256).digest()\n","\n","def getSignatureKey(key, dateStamp, regionName, serviceName):\n","    kDate = sign(('AWS4' + key).encode('utf-8'), dateStamp)\n","    kRegion = sign(kDate, regionName)\n","    kService = sign(kRegion, serviceName)\n","    kSigning = sign(kService, 'aws4_request')\n","    return kSigning"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"eIuGw4FUC-4p","executionInfo":{"status":"ok","timestamp":1762801841843,"user_tz":-60,"elapsed":8,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Crear el string para firmar\n","\n","# Si no estamos trabajando con AWS_SESSION_TOKEN\n","#canonical_headers = f'content-type:{content_type}\\nhost:{host}\\nx-amz-date:{amz_date}\\nx-amz-target:{amz_target}\\n'\n","#signed_headers = 'content-type;host;x-amz-date;x-amz-target'\n","# Si estamos trabajando con AWS_SESSION_TOKEN\n","\n","# Componemos un string con los atributos que tiene el header y sus valores, separados por \\n\n","canonical_headers = f'content-type:{content_type}\\nhost:{host}\\nx-amz-date:{amz_date}\\nx-amz-security-token:{AWS_SESSION_TOKEN}\\nx-amz-target:{amz_target}\\n'\n","\n","# le indicamos en este string qué atributos van a ser firmados\n","signed_headers = 'content-type;host;x-amz-date;x-amz-security-token;x-amz-target'\n","\n","# Obtenemos el hash del cuerpo de la petición que vamos a enviar\n","payload_hash = hashlib.sha256(request_payload.encode('utf-8')).hexdigest()\n","\n","# Componemos la canonical_request de la solicitud que se va a enviar\n","method = 'POST'  # operación HTTP que se quiere hacer\n","canonical_uri = '/'\n","canonical_querystring = ''\n","\n","canonical_request = f'{method}\\n{canonical_uri}\\n{canonical_querystring}\\n{canonical_headers}\\n{signed_headers}\\n{payload_hash}'\n"]},{"cell_type":"markdown","metadata":{"id":"AYQT1TsjC-4p"},"source":["Una **canonical request** es una representación estandarizada de la solicitud HTTP que se va a enviar a un servicio de AWS. Esta representación sigue un formato específico y es utilizada para generar la firma que se incluye en la cabecera de la solicitud.\n","\n","Una **canonical request** incluye los siguientes elementos:\n","\n","* **HTTP Method**: El método HTTP utilizado en la solicitud (por ejemplo, GET, POST, PUT, DELETE).\n","\n","* **Canonical URI**: La parte del URI de la solicitud que especifica el recurso que estás tratando de acceder (por ejemplo, /my/resource).\n","\n","* **Canonical Query String**: La cadena de consulta de la URL, en la que los parámetros están ordenados alfabéticamente y codificados de manera consistente.\n","\n","* **Canonical Headers**: Un conjunto de encabezados que se envían con la solicitud, ordenados alfabéticamente. Cada encabezado debe estar en el formato key: value, y debe terminar con un salto de línea.\n","\n","* **Signed Headers**: Una lista de los nombres de los encabezados que se han incluido en la solicitud, separados por punto y coma.\n","\n","* **Payload Hash**: Un hash (generalmente SHA-256) del cuerpo de la solicitud (payload). Para las solicitudes que no tienen cuerpo (como GET), esto suele ser el hash de una cadena vacía."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-lyULRiC-4q","executionInfo":{"status":"ok","timestamp":1762801847418,"user_tz":-60,"elapsed":13,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}},"outputId":"8fa6b100-f103-40fa-ebda-820b141a0e64"},"outputs":[{"output_type":"stream","name":"stdout","text":["POST\n","/\n","\n","content-type:application/x-amz-json-1.1\n","host:rekognition.us-east-1.amazonaws.com\n","x-amz-date:20251110T191032Z\n","x-amz-security-token:IQoJb3JpZ2luX2VjEEIaCXVzLXdlc3QtMiJIMEYCIQCwCyTHlXmoyalrOYxg3Bk5x4efG5ThHKNUA0eHNhDyNgIhAKaG+vDJBiEiIBf42V3oY0k49Qzm/JhWeRwZqenFu0jvKqACCAsQARoMMjgwMTM0OTczMzU4IgxSSbuRX1BdcMXWA+gq/QEP8DfbQ8UodiY0NrLQXadax7PKV50MFOGFazCihu5s5ZVXhWxMx9NT1PByCHC8LqfqMe+j+acO01WuUqvxDkb6Fx2S+C/g6RRO/5hVieLJIrXqilme737WFNKO/xb3qNYcTP4VT515eNRJYpwDHqiHEz1cOIXg73fzzpAYFh54acLgdsT3ZqdOHKrv3W+XauyzpxAGfymSAe5Jy3nmP25+umVRrmAsXqgjKBrMjTBknOSxmdO2nVTdLoA2aOOcmFH7UZ4BOKbSXDoJkEqUya/1GXOvYOsdvDRnSxv5iHcgaT44uzuAtFBxDEmfKFCNuYx0Fs9d7irFAddK8PMbMN3AyMgGOpwB2dBzA4CcZQ8kuWePZHLJD8pImLZyZ0lwdyT9je5avnbu9M9RrBC4i5bsJBw0Dh3f2l6+bDvN2z0NnkrfIhFPdPiqrtTw+k2axpQ38NrMoK/JMoyC2D0dp+ptDl0bC9I+0guac2fPCghxbBa8DMkG8Wr8/+8jIogoETebcNk+ykV8cIFumQq7BUaOTlR2K4OiXy3SOYbgVLxqvKP/\n","x-amz-target:RekognitionService.DetectFaces\n","\n","content-type;host;x-amz-date;x-amz-security-token;x-amz-target\n","ab4d48907988c39af55958a58bc674ac8daed0c8e8155e8bdaa7b3dc48c021d1\n"]}],"source":["print(canonical_request)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"13RR61rNC-4q","executionInfo":{"status":"ok","timestamp":1762801852037,"user_tz":-60,"elapsed":9,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Crear la string para la firma\n","algorithm = 'AWS4-HMAC-SHA256'                                        # algoritmo usado para la firma\n","service = 'rekognition'                                               # servicio de AWS al que quiero invocar\n","credential_scope = f'{date_stamp}/{REGION}/{service}/aws4_request'\n","string_to_sign = f'{algorithm}\\n{amz_date}\\n{credential_scope}\\n{hashlib.sha256(canonical_request.encode(\"utf-8\")).hexdigest()}'\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"bRCxfq7OC-4q","executionInfo":{"status":"ok","timestamp":1762801854498,"user_tz":-60,"elapsed":5,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Firmar la solicitud\n","signing_key = getSignatureKey(AWS_SECRET_KEY, date_stamp, REGION, service)\n","signature = hmac.new(signing_key, (string_to_sign).encode('utf-8'), hashlib.sha256).hexdigest()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"uM1svp6mC-4q","executionInfo":{"status":"ok","timestamp":1762801858438,"user_tz":-60,"elapsed":7,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Agregar la firma a los encabezados\n","authorization_header = f'{algorithm} Credential={AWS_ACCESS_KEY}/{credential_scope}, SignedHeaders={signed_headers}, Signature={signature}'\n","headers['Authorization'] = authorization_header"]},{"cell_type":"markdown","metadata":{"id":"3B4UsOPoC-4r"},"source":["<h4 style=\"color:orange;\">Paso 4 (Hacemos la llamada y obtenemos la respuesta)</h4>"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"EtsAK4lAC-4r","executionInfo":{"status":"ok","timestamp":1762801862868,"user_tz":-60,"elapsed":1210,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Hacer la solicitud POST\n","response = requests.post(rekognition_url, headers=headers, data=request_payload)\n"]},{"cell_type":"markdown","metadata":{"id":"ZPY704X7C-4r"},"source":["<h4 style=\"color:orange;\">Paso 5 Procesamos la respuesta)</h4>"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJEGXOM-C-4r","executionInfo":{"status":"ok","timestamp":1762801863897,"user_tz":-60,"elapsed":6,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}},"outputId":"4d5abeb1-b8f0-4cbe-aa68-2dc617b5de23"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"FaceDetails\": [\n","    {\n","      \"AgeRange\": {\n","        \"High\": 30,\n","        \"Low\": 24\n","      },\n","      \"Beard\": {\n","        \"Confidence\": 99.06271362304688,\n","        \"Value\": false\n","      },\n","      \"BoundingBox\": {\n","        \"Height\": 0.7585114240646362,\n","        \"Left\": 0.19332367181777954,\n","        \"Top\": 0.1455496996641159,\n","        \"Width\": 0.5425159335136414\n","      },\n","      \"Confidence\": 99.99896240234375,\n","      \"Emotions\": [\n","        {\n","          \"Confidence\": 100.0,\n","          \"Type\": \"HAPPY\"\n","        },\n","        {\n","          \"Confidence\": 0.028818845748901367,\n","          \"Type\": \"SURPRISED\"\n","        },\n","        {\n","          \"Confidence\": 0.0051021575927734375,\n","          \"Type\": \"CALM\"\n","        },\n","        {\n","          \"Confidence\": 0.0026226043701171875,\n","          \"Type\": \"CONFUSED\"\n","        },\n","        {\n","          \"Confidence\": 0.0018775463104248047,\n","          \"Type\": \"DISGUSTED\"\n","        },\n","        {\n","          \"Confidence\": 0.0003039836883544922,\n","          \"Type\": \"SAD\"\n","        },\n","        {\n","          \"Confidence\": 0.0002384185791015625,\n","          \"Type\": \"FEAR\"\n","        },\n","        {\n","          \"Confidence\": 0.0001728534698486328,\n","          \"Type\": \"ANGRY\"\n","        }\n","      ],\n","      \"EyeDirection\": {\n","        \"Confidence\": 99.99891662597656,\n","        \"Pitch\": 3.1645760536193848,\n","        \"Yaw\": -0.45457643270492554\n","      },\n","      \"Eyeglasses\": {\n","        \"Confidence\": 99.99925994873047,\n","        \"Value\": false\n","      },\n","      \"EyesOpen\": {\n","        \"Confidence\": 99.82601165771484,\n","        \"Value\": true\n","      },\n","      \"FaceOccluded\": {\n","        \"Confidence\": 99.8426742553711,\n","        \"Value\": false\n","      },\n","      \"Gender\": {\n","        \"Confidence\": 99.98287963867188,\n","        \"Value\": \"Female\"\n","      },\n","      \"Landmarks\": [\n","        {\n","          \"Type\": \"eyeLeft\",\n","          \"X\": 0.3720873296260834,\n","          \"Y\": 0.46672186255455017\n","        },\n","        {\n","          \"Type\": \"eyeRight\",\n","          \"X\": 0.6235265135765076,\n","          \"Y\": 0.4662960469722748\n","        },\n","        {\n","          \"Type\": \"mouthLeft\",\n","          \"X\": 0.3906378448009491,\n","          \"Y\": 0.7228468060493469\n","        },\n","        {\n","          \"Type\": \"mouthRight\",\n","          \"X\": 0.6005100607872009,\n","          \"Y\": 0.7218956351280212\n","        },\n","        {\n","          \"Type\": \"nose\",\n","          \"X\": 0.5222544074058533,\n","          \"Y\": 0.6066378355026245\n","        },\n","        {\n","          \"Type\": \"leftEyeBrowLeft\",\n","          \"X\": 0.2693493366241455,\n","          \"Y\": 0.40501973032951355\n","        },\n","        {\n","          \"Type\": \"leftEyeBrowRight\",\n","          \"X\": 0.4339401423931122,\n","          \"Y\": 0.3904750943183899\n","        },\n","        {\n","          \"Type\": \"leftEyeBrowUp\",\n","          \"X\": 0.3565714359283447,\n","          \"Y\": 0.3762435019016266\n","        },\n","        {\n","          \"Type\": \"rightEyeBrowLeft\",\n","          \"X\": 0.5773839950561523,\n","          \"Y\": 0.3906327486038208\n","        },\n","        {\n","          \"Type\": \"rightEyeBrowRight\",\n","          \"X\": 0.7055734395980835,\n","          \"Y\": 0.4044095575809479\n","        },\n","        {\n","          \"Type\": \"rightEyeBrowUp\",\n","          \"X\": 0.6455163955688477,\n","          \"Y\": 0.3763557970523834\n","        },\n","        {\n","          \"Type\": \"leftEyeLeft\",\n","          \"X\": 0.32381322979927063,\n","          \"Y\": 0.4649379849433899\n","        },\n","        {\n","          \"Type\": \"leftEyeRight\",\n","          \"X\": 0.42129918932914734,\n","          \"Y\": 0.4686122536659241\n","        },\n","        {\n","          \"Type\": \"leftEyeUp\",\n","          \"X\": 0.3726756274700165,\n","          \"Y\": 0.4537542164325714\n","        },\n","        {\n","          \"Type\": \"leftEyeDown\",\n","          \"X\": 0.3729136288166046,\n","          \"Y\": 0.4778653085231781\n","        },\n","        {\n","          \"Type\": \"rightEyeLeft\",\n","          \"X\": 0.5730236172676086,\n","          \"Y\": 0.4684391915798187\n","        },\n","        {\n","          \"Type\": \"rightEyeRight\",\n","          \"X\": 0.6641759276390076,\n","          \"Y\": 0.4642762541770935\n","        },\n","        {\n","          \"Type\": \"rightEyeUp\",\n","          \"X\": 0.6249175667762756,\n","          \"Y\": 0.45338886976242065\n","        },\n","        {\n","          \"Type\": \"rightEyeDown\",\n","          \"X\": 0.621758222579956,\n","          \"Y\": 0.4773160219192505\n","        },\n","        {\n","          \"Type\": \"noseLeft\",\n","          \"X\": 0.45826297998428345,\n","          \"Y\": 0.6326730847358704\n","        },\n","        {\n","          \"Type\": \"noseRight\",\n","          \"X\": 0.5509493947029114,\n","          \"Y\": 0.6324642896652222\n","        },\n","        {\n","          \"Type\": \"mouthUp\",\n","          \"X\": 0.5064120292663574,\n","          \"Y\": 0.6938204169273376\n","        },\n","        {\n","          \"Type\": \"mouthDown\",\n","          \"X\": 0.5019433498382568,\n","          \"Y\": 0.7704744935035706\n","        },\n","        {\n","          \"Type\": \"leftPupil\",\n","          \"X\": 0.3720873296260834,\n","          \"Y\": 0.46672186255455017\n","        },\n","        {\n","          \"Type\": \"rightPupil\",\n","          \"X\": 0.6235265135765076,\n","          \"Y\": 0.4662960469722748\n","        },\n","        {\n","          \"Type\": \"upperJawlineLeft\",\n","          \"X\": 0.18241488933563232,\n","          \"Y\": 0.4631844758987427\n","        },\n","        {\n","          \"Type\": \"midJawlineLeft\",\n","          \"X\": 0.2368186116218567,\n","          \"Y\": 0.742505669593811\n","        },\n","        {\n","          \"Type\": \"chinBottom\",\n","          \"X\": 0.4893912672996521,\n","          \"Y\": 0.9020494222640991\n","        },\n","        {\n","          \"Type\": \"midJawlineRight\",\n","          \"X\": 0.6798105239868164,\n","          \"Y\": 0.7418162822723389\n","        },\n","        {\n","          \"Type\": \"upperJawlineRight\",\n","          \"X\": 0.7288922667503357,\n","          \"Y\": 0.4628516733646393\n","        }\n","      ],\n","      \"MouthOpen\": {\n","        \"Confidence\": 99.84619903564453,\n","        \"Value\": true\n","      },\n","      \"Mustache\": {\n","        \"Confidence\": 99.90489959716797,\n","        \"Value\": false\n","      },\n","      \"Pose\": {\n","        \"Pitch\": 0.02054401859641075,\n","        \"Roll\": 0.20720067620277405,\n","        \"Yaw\": 6.804199695587158\n","      },\n","      \"Quality\": {\n","        \"Brightness\": 73.67445373535156,\n","        \"Sharpness\": 97.45164489746094\n","      },\n","      \"Smile\": {\n","        \"Confidence\": 99.80865478515625,\n","        \"Value\": true\n","      },\n","      \"Sunglasses\": {\n","        \"Confidence\": 99.9997787475586,\n","        \"Value\": false\n","      }\n","    }\n","  ]\n","}\n"]}],"source":["# Verificar la respuesta\n","if response.status_code == 200:\n","    response_json = response.json()\n","    print(json.dumps(response_json, indent=2))\n","else:\n","    print(f'Error: {response.status_code}')\n","    print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"vb3lrIQ7C-4r"},"source":["### Estructura típica del JSON de respuesta de Amazon Rekognition\n","\n","Cuando utilizas el servicio DetectLabels de Amazon Rekognition, el JSON de respuesta incluye información sobre las etiquetas (labels) detectadas en la imagen, junto con detalles como el nivel de confianza en cada etiqueta y las posibles coordenadas de los objetos.\n","\n","\n","Cómo interpretar este JSON:\n","* **Labels**: Es una lista de objetos. Cada objeto representa una etiqueta que Rekognition ha identificado en la imagen. Cada etiqueta tiene varios campos importantes:\n","    - **Name**: El nombre de la etiqueta detectada (por ejemplo, \"Person\", \"Car\").\n","    - **Confidence**: El nivel de confianza de la etiqueta, es decir, cuán segura está Rekognition de que esa etiqueta es correcta.\n","    - **Instances**: Si hay objetos detectados asociados con esta etiqueta, Instances contiene una lista de esos objetos. Cada instancia puede incluir un BoundingBox, que proporciona las coordenadas de la caja delimitadora (para resaltar el objeto en la imagen).\n","    - **Parents**: Algunas etiquetas tienen \"padres\". Por ejemplo, la etiqueta \"Car\" puede tener \"Vehicle\" como etiqueta padre, lo que indica que un auto es un tipo de vehículo.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Sbb1FUfJC-4s","executionInfo":{"status":"ok","timestamp":1762801884997,"user_tz":-60,"elapsed":45,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}}},"outputs":[],"source":["# Si estoy usando el servicio DetectLabels\n","if (amz_target=='RekognitionService.DetectLabels'):\n","    # Recorrer las etiquetas y mostrar el nombre y la confianza\n","    for label in response_json['Labels']:\n","        name = label['Name']\n","        confidence = label['Confidence']\n","        parents = label['Parents']\n","\n","        print(f\"Etiqueta: {name}, Confianza: {confidence}, Padres : {parents}%\")\n","\n","        # Si hay objetos detectados, mostrar las coordenadas de la BoundingBox\n","        if 'Instances' in label and label['Instances']:\n","            for instance in label['Instances']:\n","                box = instance['BoundingBox']\n","                print(f\"  BoundingBox - Izquierda: {box['Left']}, Superior: {box['Top']}, Ancho: {box['Width']}, Altura: {box['Height']}\")"]},{"cell_type":"markdown","metadata":{"id":"klQ83EDyC-4s"},"source":["Amazon Rekognition ofrece varios servicios avanzados además de DetectLabels. Cada uno de estos servicios utiliza el análisis de imágenes o videos basado en inteligencia artificial para resolver distintos problemas relacionados con la visión por computadora. Aquí tienes un resumen de los servicios que puedes utilizar en Amazon Rekognition:\n","\n","1. **DetectLabels (Detección de Etiquetas)**\n","Descripción: Detecta objetos, escenas y conceptos dentro de imágenes. Esto incluye cosas como \"persona\", \"auto\", \"animal\", \"edificio\", etc., junto con una confianza asociada.\n","Aplicación: Usado para análisis general de imágenes, etiquetado de contenido visual y búsqueda por etiquetas.\n","\n","2. **DetectFaces (Detección de Rostros)**\n","Descripción: Detecta rostros en imágenes y proporciona detalles como las características faciales (ojos, nariz, boca), emociones (felicidad, tristeza, sorpresa), género estimado, y más.\n","Aplicación: Útil para análisis facial, reconocimiento de emociones, o para la detección general de rostros en imágenes y videos.\n","Datos devueltos: Coordenadas de las características faciales, emociones, edad aproximada, etc.\n","\n","3. **CompareFaces (Comparación de Rostros)**\n","Descripción: Compara dos rostros para determinar si pertenecen a la misma persona. Devuelve una similitud basada en un porcentaje de confianza.\n","Aplicación: Usado para verificación de identidad, aplicaciones de autenticación o comparación entre rostros en diferentes imágenes.\n","Ejemplo: Comparar una foto de pasaporte con una selfie para verificar identidad.\n","\n","4. **RecognizeCelebrities (Reconocimiento de Celebridades)**\n","Descripción: Identifica celebridades dentro de una imagen. Devuelve nombres de las personas reconocidas junto con un nivel de confianza.\n","Aplicación: Usado en medios, entretenimiento, y aplicaciones que requieren reconocer celebridades en imágenes o videos.\n","\n","5. **DetectText (Detección de Texto)**\n","Descripción: Detecta y extrae texto de imágenes. Puede identificar texto impreso o escrito a mano, y también permite localizar la posición del texto dentro de la imagen.\n","Aplicación: Reconocimiento óptico de caracteres (OCR) en imágenes, análisis de documentos escaneados, señales, posters o cualquier otra fuente de texto dentro de imágenes.\n","Datos devueltos: Frases, palabras, y sus coordenadas dentro de la imagen.\n","\n","6. **DetectModerationLabels (Etiquetas de Moderación de Contenido)**\n","Descripción: Detecta contenido potencialmente ofensivo o inapropiado en imágenes, como desnudos, violencia, contenido sugestivo o drogas.\n","Aplicación: Moderación automática de contenido para plataformas de redes sociales, sitios de video o cualquier aplicación que necesite filtrar contenido sensible.\n","Etiquetas: \"Nudity\", \"Violence\", \"Explicit Nudity\", entre otras.\n","\n","7. **FaceSearch (Búsqueda de Rostros en Colección)**\n","Descripción: Busca rostros en una imagen o video en una colección de rostros predefinida. Es útil para identificar personas que ya han sido almacenadas en una base de datos de rostros.\n","Aplicación: Reconocimiento facial en tiempo real para seguridad, acceso a sistemas o identificación en grandes multitudes.\n","Colecciones: Se pueden crear colecciones de rostros y realizar búsquedas rápidas para encontrar coincidencias.\n","\n","8. **CreateCollection y DeleteCollection (Gestión de Colecciones de Rostros)**\n","Descripción: Permite crear y gestionar colecciones de rostros. Una colección de rostros es un conjunto de imágenes de rostros que puede ser utilizado para comparación o búsqueda en otros servicios como FaceSearch.\n","Aplicación: Crear una base de datos para reconocimiento facial a nivel empresarial o para aplicaciones como control de acceso.\n","\n","9. **IndexFaces (Indexación de Rostros)**\n","Descripción: Indexa los rostros detectados en una imagen y los almacena en una colección. Los rostros indexados pueden ser buscados y comparados usando SearchFaces o SearchFacesByImage.\n","Aplicación: Identificación de personas a partir de una base de datos de rostros (por ejemplo, en control de acceso o vigilancia).\n","\n","10. **SearchFaces y SearchFacesByImage (Búsqueda de Rostros)**\n","Descripción: Permite buscar rostros similares dentro de una colección de rostros indexados. SearchFacesByImage busca en una colección a partir de una imagen dada.\n","Aplicación: Reconocimiento de personas dentro de una colección almacenada, para aplicaciones de control de acceso, seguridad, o verificación de identidad.\n","\n","11. **DetectProtectiveEquipment (Detección de Equipos de Protección Personal)**\n","Descripción: Detecta si las personas en una imagen están usando equipo de protección personal (PPE), como cascos, máscaras o chalecos. También identifica si el equipo está siendo utilizado correctamente (por ejemplo, un casco en la cabeza).\n","Aplicación: Cumplimiento de seguridad laboral, supervisión en obras de construcción o fábricas para verificar que los empleados usen equipo de seguridad.\n","\n","12. **AnalyzeFaces (Análisis Facial Profundo)**\n","Descripción: Proporciona análisis avanzado de características faciales, emociones, género estimado, y edad estimada, además de detalles específicos como si la persona está sonriendo o si tiene los ojos abiertos.\n","Aplicación: Publicidad segmentada, análisis de experiencias de usuario, o para cualquier aplicación que necesite analizar cómo se sienten las personas.\n","\n","13. **DetectCustomLabels (Etiquetas Personalizadas)**\n","Descripción: Permite a los usuarios entrenar modelos personalizados para detectar objetos o escenarios específicos. Este servicio es útil cuando necesitas etiquetar objetos que no están cubiertos por las etiquetas prediseñadas de Rekognition.\n","Aplicación: Reconocimiento personalizado para casos de uso específicos de la industria, como la detección de productos, logotipos, o cualquier otro objeto que no esté cubierto por las etiquetas estándar.\n","\n","14. **DetectTextInVideo (Detección de Texto en Video)**\n","Descripción: Similar a DetectText, pero aplicado en secuencias de video, donde se detecta texto impreso o escrito a mano en cada fotograma de video.\n","Aplicación: Análisis de videos que contienen señales, subtítulos o cualquier otra forma de texto.\n","\n","15. **Video Segment Analysis (Análisis de Segmentos de Video)**\n","Descripción: Análisis de videos para detectar escenas, personas, objetos, rostros y más dentro de un flujo de video.\n","Aplicación: Procesamiento de videos para identificar automáticamente personas u objetos a lo largo del tiempo."]},{"cell_type":"markdown","metadata":{"id":"89iyWiWRC-4s"},"source":["Por ejemplo, prueba a a usar el servicio DetectFaces, en lugar de DetectLabels\n","\n","amz_target = 'RekognitionService.DetectLabels'\n","\n","se cambia por:\n","\n","amz_target = 'RekognitionService.DetectFaces'\n"]},{"cell_type":"markdown","metadata":{"id":"anLuAVI9C-4t"},"source":["En este caso, cambia el formato del JSON de salida. Podríamos recorrerlo con:"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NoxYlxpC-4t","executionInfo":{"status":"ok","timestamp":1762801890528,"user_tz":-60,"elapsed":12,"user":{"displayName":"Isabel María Espejo Delgado","userId":"10712750474963986236"}},"outputId":"c2939737-7847-4261-be20-c2ea8ac1d5cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Rango de edad: 24 - 30\n","Género: Female\n","Sonrisa: Sí\n","Emociones:\n","  HAPPY: 100.0%\n","  SURPRISED: 0.028818845748901367%\n","  CALM: 0.0051021575927734375%\n","  CONFUSED: 0.0026226043701171875%\n","  DISGUSTED: 0.0018775463104248047%\n","  SAD: 0.0003039836883544922%\n","  FEAR: 0.0002384185791015625%\n","  ANGRY: 0.0001728534698486328%\n"]}],"source":["# Si estoy usando el servicio DetectFaces, puedo acceder a ciertos datos como su edad estimada, género, emociones, etc, de la siguiente forma:\n","if (amz_target=='RekognitionService.DetectFaces'):\n","    for face in response_json['FaceDetails']:\n","        age_range = face['AgeRange']\n","        gender = face['Gender']['Value']\n","        smile = face['Smile']['Value']\n","        emotions = face['Emotions']\n","\n","        print(f\"Rango de edad: {age_range['Low']} - {age_range['High']}\")\n","        print(f\"Género: {gender}\")\n","        print(f\"Sonrisa: {'Sí' if smile else 'No'}\")\n","\n","        # Emociones\n","        print(\"Emociones:\")\n","        for emotion in emotions:\n","            print(f\"  {emotion['Type']}: {emotion['Confidence']}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7-lXqEOC-4t"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}